================================================================================
FINALIZED DESIGN ARCHITECTURE
Custom POMO with Global Edge Selection and Deletion for TSP
================================================================================

USER PROMPT: User answers to questions in DESIGN_QEUSTIONS2.txt. 1, Theta should be absolute, east=0 degrees. 2, Please include aboslute distances to the neighbors, the problem will already be normalized appropriatly. 3, For the polar coordinates for two connected edges, lets put the closer connection first. Ok, I think we have answered a lot of questions. Please create a final, DESIGN_ARCHITECTURE.txt file that will document how we want to proceed with development. There will be opportunities to ask more questions as we advance through the development setps outlines in the USER_DEVELOPMENT_PLAN.txt. We are currently in setp 1, and hopefully progressing to step 2.

USER RESPONSE 1: Ok, I think your design architecture completes step two in the user development plan, and we can proceed with step 3. Please execute step 3 of the user development plan. If at any point you need to ask additional question about this step, please aggregate all your questions into a DESIGN_QUESTIONS_MODEL.txt file, and we can have additional dialogue.

USER RESPONSE 2: You must access the rl4co repository frequently to understand the library's capabilities, classes, functions, and ecosystem. Rely on the offical documents heavily.

Date: 2025-11-22
Status: FINALIZED - Ready for Implementation (Step 2)
Framework: RL4CO (use native classes and functions wherever possible)

Project Goal:
  Develop a deep learning model that solves TSP using sequential actions to
  select or remove edges. The model considers the problem globally when 
  selecting edges and can delete edges to correct mistakes.

Key Innovation:
  - Global edge selection (not autoregressive from current position)
  - Edge deletion capability (allows backtracking/correction)
  - Multi-head decoder architecture (ADD, DELETE, DONE)
  - Node-centric state representation with polar coordinate features


================================================================================
PART 1: ENVIRONMENT SPECIFICATION
================================================================================

CLASS: CustomTSPEnv (extends rl4co.envs.RL4COEnvBase)
-----------------------------------------------------

STATE REPRESENTATION:
--------------------
The environment maintains and exposes the following state:

1. Core Problem Data:
   - locs: (batch, N, 2) - Node coordinates (x, y)
   - N: Number of nodes in the problem

2. Solution State:
   - adjacency: (batch, N, N) - Binary matrix, A[i,j]=1 if edge (i,j) selected
   - degrees: (batch, N) - Count of selected edges per node, values in {0,1,2}
   - selected_edges: (batch, max_edges, 2) - List of selected edge indices
   - num_edges: (batch,) - Count of currently selected edges
   
   IMPLEMENTATION NOTE: Store BOTH adjacency matrix AND edge list
     - Adjacency: Fast lookups (O(1)), degree tracking, action masking
     - Edge list: Efficient iteration for DELETE masking, reward calculation
     - Memory cost minimal for N≤100 (~40KB), worth the performance gain

3. Episode Progress:
   - current_step: (batch,) - Step counter, range [0, 2*N]
   - num_deletions: (batch,) - Count of deletion actions taken
   - action_history: (batch, 3, 3) - Last 3 actions as (type, node_i, node_j)
     where type: {0=ADD, 1=DELETE, 2=DONE}

4. POMO-Specific:
   - starting_edge: (batch, 2) - Initial edge for this trajectory
   - trajectory_id: (batch,) - Which POMO trajectory this is

5. Computed Features (for policy network):
   - node_features: (batch, N, feat_dim) - See NODE FEATURES section below
   
   PHASED IMPLEMENTATION:
     Step 3 (Environment): Core features only (locs, adjacency, degrees, counters)
     Step 6 (Policy): Add polar coordinates, action history when needed by network
     Rationale: Defer expensive feature computation until policy requires it


NODE FEATURES (per node i):
---------------------------
Total dimension: ~20 features per node

1-2.   x_i, y_i: Node coordinates (normalized to [0,1])
3.     degree_i: Current degree (0, 1, or 2)
4.     theta_1: Polar angle to 1st connected neighbor (radians, [0, 2π])
           - Absolute reference: East = 0°, counterclockwise
           - 0 if no connections exist
5.     d_1: Euclidean distance to 1st connected neighbor (absolute)
           - 0 if no connections exist
6.     theta_2: Polar angle to 2nd connected neighbor (radians, [0, 2π])
           - 0 if degree < 2
7.     d_2: Euclidean distance to 2nd connected neighbor (absolute)
           - 0 if degree < 2
8.     step_normalized: current_step / (2*N)
9.     deletions_normalized: num_deletions / N
10-18. action_history_embedding: Embedded representation of last 3 actions
           - Each action embedded as (type, node_i, node_j)
           - Summed or concatenated (~9 dimensions)
19.    positional_encoding: Learned or fixed position embedding
20.    is_starting_node: Binary flag if node is part of starting edge

Notes on polar coordinates:
  - When node has 2 connections, order by DISTANCE (closer first)
  - theta_1 corresponds to closer neighbor, theta_2 to farther
  - Compute theta relative to node's position: theta = atan2(dy, dx)
  - Distances are absolute (already normalized by problem scale)


ACTION SPACE:
------------
Three types of actions:

INTERNAL REPRESENTATION: Single integer action index (rl4co standard)
  - Index 0 to N*(N-1)/2-1: ADD actions (for each pair i<j)
  - Index N*(N-1)/2 to N*(N-1)/2+num_edges-1: DELETE actions
  - Index N*(N-1)/2+num_edges: DONE action
  - Decoded internally to (action_type, node_i, node_j) for execution
  - Helper functions: decode_action(), encode_action() for testing/visualization

1. ADD(i, j): Add edge between nodes i and j
   - Representation: action_type=0, node_1=i, node_2=j where i < j
   - Effect: Set adjacency[i,j] = adjacency[j,i] = 1
             Increment degrees[i] and degrees[j]
             Update node features for i and j (and their neighbors)
   - Count: ~N*(N-1)/2 possible actions (before masking)

2. DELETE(i, j): Remove edge between nodes i and j
   - Representation: action_type=1, node_1=i, node_2=j
   - Effect: Set adjacency[i,j] = adjacency[j,i] = 0
             Decrement degrees[i] and degrees[j]
             Update node features for i and j (and their neighbors)
             Increment num_deletions counter
   - Count: Up to N possible actions (currently selected edges)

3. DONE: Terminate episode
   - Representation: action_type=2, node_1=-1, node_2=-1
   - Effect: End episode, compute final reward
   - Count: 1 action


ACTION MASKING:
--------------
Hard constraints enforced via masking:

For ADD(i, j):
  - Mask if edge (i,j) already exists: adjacency[i,j] == 1
  - Mask if node i has degree 2: degrees[i] == 2
  - Mask if node j has degree 2: degrees[j] == 2
  - Mask if i == j (self-loop)
  - Always mask: i >= j (enforce i < j convention for undirected edges)

For DELETE(i, j):
  - Mask if edge (i,j) doesn't exist: adjacency[i,j] == 0
  - Mask if (i,j) is the starting edge (POMO constraint)

For DONE:
  - Mask if tour is invalid:
    * Any node has degree != 2, OR
    * Graph is not connected (use BFS/DFS check)
  - Only check connectivity when all degrees == 2 (optimization)
  
  IMPLEMENTATION: Connectivity check optimization
    - Check degrees every step: O(N), very fast
    - Run BFS only when degrees == 2: O(N), rare event
    - Amortized cost is negligible, keeps code simple
    - Alternative: Union-Find tracking (defer unless profiling shows need)


DYNAMICS (step function):
-------------------------
```
def step(state, action):
    action_type, node_i, node_j = parse_action(action)
    
    if action_type == ADD:
        state.adjacency[node_i, node_j] = 1
        state.adjacency[node_j, node_i] = 1
        state.degrees[node_i] += 1
        state.degrees[node_j] += 1
        state.num_edges += 1
        
        # WARNING: Check for degree > 2 (should be prevented by masking)
        if state.degrees[node_i] > 2:
            log.warning(f"Node {node_i} has degree {state.degrees[node_i]} > 2! "
                       f"Should be prevented by action masking.")
        if state.degrees[node_j] > 2:
            log.warning(f"Node {node_j} has degree {state.degrees[node_j]} > 2! "
                       f"Should be prevented by action masking.")
        
    elif action_type == DELETE:
        state.adjacency[node_i, node_j] = 0
        state.adjacency[node_j, node_i] = 0
        state.degrees[node_i] -= 1
        state.degrees[node_j] -= 1
        state.num_edges -= 1
        state.num_deletions += 1
        
    elif action_type == DONE:
        state.done = True
        
        # WARNING: Check if tour is valid (should be prevented by masking)
        if not all(state.degrees == 2):
            invalid_nodes = [i for i in range(N) if state.degrees[i] != 2]
            log.warning(f"DONE called with invalid degrees! "
                       f"Nodes with degree != 2: {invalid_nodes}. "
                       f"Should be prevented by action masking.")
        elif not is_connected(state.adjacency):
            log.warning(f"DONE called with disconnected graph! "
                       f"All nodes have degree 2 but graph not connected. "
                       f"Should be prevented by action masking.")
        
    # Update derived state
    state.current_step += 1
    update_node_features(state)  # Recompute polar coords, etc. (deferred to Step 6)
    update_action_history(state, action)  # (deferred to Step 6)
    
    # Logging (conditional based on log level)
    # DEBUG: Log every action details
    # INFO: Log episode summaries only
    # WARNING: Log only issues/anomalies
    
    # Check termination
    if state.current_step >= 2*N:
        state.done = True
        state.hit_step_limit = True
    
    # Compute reward if done
    if state.done:
        reward = compute_reward(state)
    else:
        reward = 0  # Sparse reward
        
    return state, reward
```


REWARD FUNCTION:
---------------
Sparse reward, computed only at episode termination:

IMPLEMENTATION NOTE: Use raw (unnormalized) reward values initially
  - Typical tour length for N=50 uniform [0,1]: ~5-7
  - Deletion penalty (0.2% avg dist): ~0.001 per deletion  
  - rl4co models handle raw tour lengths well
  - Monitor reward magnitudes during training
  - Add normalization later if learning is unstable

```
def compute_reward(state):
    # Check if tour is valid
    valid = check_validity(state)
    
    if not valid:
        return -10000  # Large penalty for invalid tour
    
    # Compute tour length
    tour_length = compute_tour_length(state.adjacency, state.locs)
    
    # Deletion penalty: 0.2% of average edge distance per deletion
    avg_dist = compute_avg_distance(state.locs)
    deletion_penalty = 0.002 * avg_dist * state.num_deletions
    
    # Step limit penalty if applicable
    step_limit_penalty = -5000 if state.hit_step_limit else 0
    
    # Final reward (negative for minimization)
    reward = -(tour_length + deletion_penalty) + step_limit_penalty
    
    return reward

def check_validity(state):
    # All nodes must have degree 2
    if not all(state.degrees == 2):
        return False
    
    # Graph must be connected (single cycle)
    if not is_connected(state.adjacency):
        return False
    
    return True

def is_connected(adjacency):
    # BFS from node 0, check if all nodes reachable
    N = len(adjacency)
    visited = bfs(adjacency, start=0)
    return len(visited) == N
```


RESET (initialization):
-----------------------
```
def reset(batch_size, N, pomo_size=8):
    # Generate random TSP instances
    locs = sample_uniform(batch_size, N, 2)  # Coordinates in [0,1]
    
    # Initialize state
    state = TensorDict({
        'locs': locs,
        'adjacency': zeros(batch_size, N, N),
        'degrees': zeros(batch_size, N),
        'current_step': zeros(batch_size),
        'num_deletions': zeros(batch_size),
        'num_edges': zeros(batch_size),
        'done': zeros(batch_size, dtype=bool),
        'hit_step_limit': zeros(batch_size, dtype=bool),
    })
    
    # POMO augmentation: create multiple trajectories with different starts
    if pomo_size > 1:
        state = expand_for_pomo(state, pomo_size)
        starting_edges = select_diverse_edges(state, pomo_size)
        state['starting_edge'] = starting_edges
        
        # Pre-add starting edges
        for traj_id in range(pomo_size):
            i, j = starting_edges[traj_id]
            state.adjacency[traj_id, i, j] = 1
            state.adjacency[traj_id, j, i] = 1
            state.degrees[traj_id, i] = 1
            state.degrees[traj_id, j] = 1
            state.num_edges[traj_id] = 1
    
    # Compute initial node features
    update_node_features(state)
    
    return state

def select_diverse_edges(state, K):
    # MINIMAL VIABLE: Simple round-robin by index
    # Edge k = nodes (k, (k+K) mod N)
    # PRO: Fast, deterministic, ensures no overlap
    # FUTURE: Upgrade to greedy diversity maximization if needed
    #   - Iteratively select edges maximizing min distance to existing
    #   - Use edge midpoints for distance calculation
    edges = [(k, (k + K) % N) for k in range(K)]
    return torch.tensor(edges)
```


================================================================================
PART 2: POLICY NETWORK SPECIFICATION
================================================================================

CLASS: CustomPOMOPolicy (implemented in tsp_custom/models/)
-----------------------------------------------------------
Implementation Status: ✓ COMPLETED (Step 6)
Files: encoder.py, decoders.py, custom_pomo_policy.py, action_utils.py

ARCHITECTURE OVERVIEW:
---------------------
```
Input: TensorDict with state (locs, adjacency, degrees, counters)
    ↓
Create Node Features: (batch, N, N+5)
  [locs(2), degree(1), adjacency_row(N), step_norm(1), deletion_norm(1)]
    ↓
ENCODER: TransformerEncoder (6 layers)
  - embed_dim=128, num_heads=8, feedforward_dim=512
  - Instance normalization, residual connections
    ↓
Node Embeddings: (batch, N, 128)
    ↓
    ├──→ AddEdgeDecoder       → logits_add (batch, N*(N-1)/2)
    ├──→ DeleteEdgeDecoder    → logits_del (batch, max_edges)  
    └──→ DoneDecoder          → logit_done (batch, 1)
    ↓
Concatenate: (batch, N*(N-1)/2 + max_edges + 1)
    ↓
Apply tanh clipping: tanh(logits) * 10.0
    ↓
Apply action mask from environment
    ↓
Temperature scaling: logits / temperature
    ↓
Softmax + Sample/Argmax
    ↓
Output: action index (batch,)
```

Total Parameters: 1,264,515 (~1.26M)
  - Encoder: ~1.1M (transformer layers)
  - Decoders: ~150K (3 MLP heads)


ENCODER IMPLEMENTATION:
----------------------
File: tsp_custom/models/encoder.py

class TransformerEncoder(nn.Module):
    """
    6-layer transformer encoder with multi-head self-attention.
    
    Args:
        feat_dim: Input feature dimension (default: N+5 for adjacency-based features)
        embed_dim: Embedding dimension (default: 128)
        num_heads: Number of attention heads (default: 8)
        num_layers: Number of transformer layers (default: 6)
        feedforward_dim: FFN hidden dimension (default: 512)
        normalization: 'instance', 'batch', or 'layer' (default: 'instance')
    """
    
    def __init__(self, feat_dim, embed_dim=128, num_heads=8, 
                 num_layers=6, feedforward_dim=512, normalization='instance'):
        super().__init__()
        
        # Initial projection: feat_dim → embed_dim
        self.init_embed = nn.Linear(feat_dim, embed_dim)
        
        # Stack of transformer layers
        self.layers = nn.ModuleList([
            TransformerEncoderLayer(
                embed_dim=embed_dim,
                num_heads=num_heads,
                feedforward_dim=feedforward_dim,
                normalization=normalization
            )
            for _ in range(num_layers)
        ])
        
    def forward(self, node_features):
        # node_features: (batch, N, feat_dim)
        h = self.init_embed(node_features)  # (batch, N, embed_dim)
        
        for layer in self.layers:
            h = layer(h)  # Multi-head attention + FFN with residuals
            
        return h  # (batch, N, embed_dim)


class TransformerEncoderLayer(nn.Module):
    """Single transformer layer with multi-head self-attention and FFN."""
    
    def __init__(self, embed_dim=128, num_heads=8, feedforward_dim=512, 
                 normalization='instance'):
        super().__init__()
        
        # Multi-head attention
        self.attn_q = nn.Linear(embed_dim, embed_dim, bias=False)
        self.attn_k = nn.Linear(embed_dim, embed_dim, bias=False)
        self.attn_v = nn.Linear(embed_dim, embed_dim, bias=False)
        self.attn_out = nn.Linear(embed_dim, embed_dim, bias=False)
        
        # Feed-forward network
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, feedforward_dim),
            nn.ReLU(),
            nn.Linear(feedforward_dim, embed_dim)
        )
        
        # Normalization layers
        if normalization == 'instance':
            self.norm1 = nn.InstanceNorm1d(embed_dim, affine=True)
            self.norm2 = nn.InstanceNorm1d(embed_dim, affine=True)
        elif normalization == 'batch':
            self.norm1 = nn.BatchNorm1d(embed_dim)
            self.norm2 = nn.BatchNorm1d(embed_dim)
        else:  # layer
            self.norm1 = nn.LayerNorm(embed_dim)
            self.norm2 = nn.LayerNorm(embed_dim)
        
    def forward(self, h):
        # Multi-head self-attention with residual
        h = h + self._multi_head_attention(h)
        h = self.norm1(h.transpose(1, 2)).transpose(1, 2)  # Normalize
        
        # Feed-forward with residual
        h = h + self.ff(h)
        h = self.norm2(h.transpose(1, 2)).transpose(1, 2)  # Normalize
        
        return h
```


DECODER IMPLEMENTATIONS:
-----------------------
File: tsp_custom/models/decoders.py

HEAD 1 - AddEdgeDecoder:
-----------------------
```python
class AddEdgeDecoder(nn.Module):
    """
    Decoder for ADD edge actions.
    Scores all potential edges (i,j) where i < j.
    
    Args:
        embed_dim: Node embedding dimension (default: 128)
        hidden_dim: MLP hidden dimension (default: 128)
    """
    
    def __init__(self, embed_dim=128, hidden_dim=128):
        super().__init__()
        
        # Edge scoring MLP: concat(emb_i, emb_j, distance) → score
        self.edge_scorer = nn.Sequential(
            nn.Linear(2 * embed_dim + 1, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
    def forward(self, node_embeddings, locs):
        # node_embeddings: (batch, N, embed_dim)
        # locs: (batch, N, 2)
        # Returns: logits_add (batch, N*(N-1)/2), edge_indices (N*(N-1)/2, 2)
        
        batch, N, embed_dim = node_embeddings.shape
        
        # Compute pairwise distances
        dist = torch.cdist(locs, locs, p=2)  # (batch, N, N)
        
        # Create pairwise embeddings via broadcasting
        emb_i = node_embeddings.unsqueeze(2).expand(batch, N, N, embed_dim)
        emb_j = node_embeddings.unsqueeze(1).expand(batch, N, N, embed_dim)
        
        # Concatenate features
        edge_features = torch.cat([
            emb_i, emb_j, dist.unsqueeze(-1)
        ], dim=-1)  # (batch, N, N, 2*embed_dim + 1)
        
        # Score all edges
        logits = self.edge_scorer(edge_features).squeeze(-1)  # (batch, N, N)
        
        # Extract upper triangle (i < j) to avoid duplicates
        triu_indices = torch.triu_indices(N, N, offset=1, device=logits.device)
        edge_indices = triu_indices.t()  # (N*(N-1)/2, 2)
        logits_add = logits[:, triu_indices[0], triu_indices[1]]  # (batch, N*(N-1)/2)
        
        return logits_add, edge_indices


HEAD 2 - DeleteEdgeDecoder:
---------------------------
class DeleteEdgeDecoder(nn.Module):
    """
    Decoder for DELETE edge actions.
    Scores existing edges in the current partial tour.
    
    Args:
        embed_dim: Node embedding dimension (default: 128)
        hidden_dim: MLP hidden dimension (default: 128)
    """
    
    def __init__(self, embed_dim=128, hidden_dim=128):
        super().__init__()
        
        # Edge scoring MLP (same structure as ADD for consistency)
        self.edge_scorer = nn.Sequential(
            nn.Linear(2 * embed_dim + 1, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
    def forward(self, node_embeddings, locs, edge_list, delete_bias=0.0):
        # node_embeddings: (batch, N, embed_dim)
        # locs: (batch, N, 2)
        # edge_list: (batch, max_edges, 2) - padded with -1
        # delete_bias: Scalar bias (scheduled -5.0 → 0.0)
        # Returns: logits_del (batch, max_edges)
        
        batch, max_edges, _ = edge_list.shape
        
        logits_del_list = []
        for b in range(batch):
            edges = edge_list[b]  # (max_edges, 2)
            valid_mask = (edges[:, 0] >= 0) & (edges[:, 1] >= 0)
            
            # Get embeddings for edge endpoints (clamp for invalid edges)
            edges_clamped = edges.clamp(min=0)
            emb_i = node_embeddings[b, edges_clamped[:, 0]]
            emb_j = node_embeddings[b, edges_clamped[:, 1]]
            
            # Compute distances
            locs_i = locs[b, edges_clamped[:, 0]]
            locs_j = locs[b, edges_clamped[:, 1]]
            dist = torch.norm(locs_i - locs_j, dim=-1, keepdim=True)
            
            # Score edges
            edge_features = torch.cat([emb_i, emb_j, dist], dim=-1)
            logits = self.edge_scorer(edge_features).squeeze(-1)
            
            # Mask invalid edges (padding)
            logits = logits.masked_fill(~valid_mask, float('-inf'))
            logits_del_list.append(logits)
        
        logits_del = torch.stack(logits_del_list)  # (batch, max_edges)
        
        # Apply delete bias (curriculum learning)
        logits_del = logits_del + delete_bias
        
        return logits_del


HEAD 3 - DoneDecoder:
--------------------
class DoneDecoder(nn.Module):
    """
    Decoder for DONE action.
    Uses global graph context to decide termination.
    
    Args:
        embed_dim: Node embedding dimension (default: 128)
        hidden_dim: MLP hidden dimension (default: 64)
    """
    
    def __init__(self, embed_dim=128, hidden_dim=64):
        super().__init__()
        
        # DONE scoring MLP
        self.scorer = nn.Sequential(
            nn.Linear(embed_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
    def forward(self, node_embeddings):
        # node_embeddings: (batch, N, embed_dim)
        # Returns: logit_done (batch, 1)
        
        # Global context: mean pooling over all nodes
        global_context = node_embeddings.mean(dim=1)  # (batch, embed_dim)
        
        # Score DONE action
        logit_done = self.scorer(global_context)  # (batch, 1)
        
        return logit_done
```


POLICY FORWARD PASS IMPLEMENTATION:
----------------------------------
File: tsp_custom/models/custom_pomo_policy.py

```python
class CustomPOMOPolicy(nn.Module):
    """
    Custom policy for TSP with global edge selection and deletion.
    
    Args:
        num_loc: Number of nodes in TSP instance
        embed_dim: Embedding dimension (default: 128)
        num_heads: Number of attention heads (default: 8)
        num_encoder_layers: Number of transformer layers (default: 6)
        feedforward_dim: FFN hidden dimension (default: 512)
        normalization: Normalization type (default: 'instance')
        temperature: Softmax temperature (default: 1.0)
        tanh_clipping: Tanh clipping value (default: 10.0)
        delete_bias_start: Initial delete bias (default: -5.0)
        delete_bias_end: Final delete bias (default: 0.0)
        delete_bias_warmup_epochs: Epochs for linear schedule (default: 100)
    """
    
    def __init__(
        self,
        num_loc: int,
        embed_dim: int = 128,
        num_heads: int = 8,
        num_encoder_layers: int = 6,
        feedforward_dim: int = 512,
        normalization: str = 'instance',
        temperature: float = 1.0,
        tanh_clipping: float = 10.0,
        delete_bias_start: float = -5.0,
        delete_bias_end: float = 0.0,
        delete_bias_warmup_epochs: int = 100,
    ):
        super().__init__()
        
        self.num_loc = num_loc
        self.embed_dim = embed_dim
        self.temperature = temperature
        self.tanh_clipping = tanh_clipping
        
        # Calculate action space sizes
        self.num_add_actions = num_loc * (num_loc - 1) // 2
        self.max_delete_actions = num_loc
        
        # Feature dimension: locs(2) + degree(1) + adjacency(N) + step(1) + deletions(1)
        self.feat_dim = num_loc + 5
        
        # Encoder
        self.encoder = TransformerEncoder(
            feat_dim=self.feat_dim,
            embed_dim=embed_dim,
            num_heads=num_heads,
            num_layers=num_encoder_layers,
            feedforward_dim=feedforward_dim,
            normalization=normalization
        )
        
        # Decoder heads
        self.add_decoder = AddEdgeDecoder(embed_dim=embed_dim)
        self.delete_decoder = DeleteEdgeDecoder(embed_dim=embed_dim)
        self.done_decoder = DoneDecoder(embed_dim=embed_dim)
        
        # Delete bias schedule
        self.register_buffer('delete_bias_start', torch.tensor(delete_bias_start))
        self.register_buffer('delete_bias_end', torch.tensor(delete_bias_end))
        self.delete_bias_warmup_epochs = delete_bias_warmup_epochs
        self.current_epoch = 0
        
        # Pre-compute edge indices for ADD actions
        edge_indices = torch.triu_indices(num_loc, num_loc, offset=1).t()
        self.register_buffer('edge_indices', edge_indices)
        
    def forward(
        self,
        td: TensorDict,
        phase: str = 'train',
        decode_type: str = 'sampling',
        return_actions: bool = False,
        return_hidden: bool = False,
    ) -> dict:
        """
        Forward pass of the policy.
        
        Args:
            td: TensorDict containing environment state
            phase: 'train', 'val', or 'test'
            decode_type: 'sampling' or 'greedy'
            return_actions: Whether to return action components
            return_hidden: Whether to return node embeddings
            
        Returns:
            Dictionary with 'action', 'log_prob', and optional fields
        """
        
        # 1. Create node features from state
        node_features = create_node_features(td, self.num_loc)  # (batch, N, N+5)
        
        # 2. Encode
        node_embeddings = self.encoder(node_features)  # (batch, N, 128)
        
        # 3. Extract edge list for DELETE decoder
        edge_list = extract_edge_list(td["adjacency"], self.max_delete_actions)
        
        # 4. Get delete bias
        delete_bias = self.get_delete_bias(phase)
        
        # 5. Decode with three heads
        logits_add, _ = self.add_decoder(node_embeddings, td["locs"])
        logits_del = self.delete_decoder(
            node_embeddings, td["locs"], edge_list, delete_bias
        )
        logits_done = self.done_decoder(node_embeddings)
        
        # 6. Concatenate all logits
        logits = torch.cat([logits_add, logits_del, logits_done], dim=-1)
        
        # 7. Apply tanh clipping
        if self.tanh_clipping > 0:
            logits = torch.tanh(logits) * self.tanh_clipping
        
        # 8. Apply action mask from environment
        mask_add, mask_del, mask_done = compute_action_masks(
            td, self.num_add_actions, self.max_delete_actions
        )
        mask = torch.cat([mask_add, mask_del, mask_done.unsqueeze(-1)], dim=-1)
        logits = logits.masked_fill(~mask, float('-inf'))
        
        # 9. Apply temperature
        logits = logits / self.temperature
        
        # 10. Sample or select action
        if decode_type == 'sampling':
            probs = F.softmax(logits, dim=-1)
            probs = torch.where(torch.isnan(probs), torch.zeros_like(probs), probs)
            action_idx = torch.multinomial(probs, 1).squeeze(-1)
        else:  # greedy
            action_idx = logits.argmax(dim=-1)
        
        # 11. Compute log probability
        log_probs = F.log_softmax(logits, dim=-1)
        selected_log_prob = log_probs.gather(-1, action_idx.unsqueeze(-1)).squeeze(-1)
        
        # 12. Decode action (for debugging, environment uses action_idx directly)
        action_type, node_i, node_j = decode_action_index(
            action_idx, self.num_add_actions, self.max_delete_actions,
            self.edge_indices, edge_list
        )
        
        # 13. Prepare output
        out = {
            "action": action_idx,  # Environment expects flat index
            "log_prob": selected_log_prob,
        }
        
        if return_actions:
            out["action_components"] = {
                "action_type": action_type,
                "node_i": node_i,
                "node_j": node_j,
            }
        
        if return_hidden:
            out["hidden"] = node_embeddings
        
        return out
    
    def get_delete_bias(self, phase: str = 'train') -> float:
        """Compute delete bias based on current epoch (linear schedule)."""
        if phase != 'train':
            return self.delete_bias_end.item()
        
        if self.current_epoch >= self.delete_bias_warmup_epochs:
            return self.delete_bias_end.item()
        
        alpha = self.current_epoch / self.delete_bias_warmup_epochs
        bias = (1 - alpha) * self.delete_bias_start + alpha * self.delete_bias_end
        return bias.item()
    
    def set_epoch(self, epoch: int):
        """Update current epoch for delete bias scheduling."""
        self.current_epoch = epoch
```


ACTION UTILITIES:
----------------
File: tsp_custom/models/action_utils.py

Key functions:
- decode_action_index(): Maps flat index → (action_type, node_i, node_j)
- extract_edge_list(): Converts adjacency matrix → edge list for DELETE
- compute_action_masks(): Splits environment mask for 3 decoder heads
- create_node_features(): Builds input features (locs, degree, adjacency, counters)

Node Feature Construction:
```python
def create_node_features(td, num_loc: int) -> torch.Tensor:
    """
    Create node features from TensorDict state.
    
    Features per node (total: N+5):
    - locs: x, y coordinates (2)
    - degree: number of connections (1)
    - adjacency_row: connections to all nodes (N)
    - step_norm: current step / max_steps (1)
    - deletion_norm: num_deletions / num_loc (1)
    
    Returns: (batch, N, N+5)
    """
    locs = td["locs"]
    adjacency = td["adjacency"]
    degrees = td["degrees"]
    current_step = td["current_step"]
    num_deletions = td["num_deletions"]
    
    # Normalize counters
    max_steps = 2 * num_loc
    step_norm = current_step.float() / max_steps
    deletion_norm = num_deletions.float() / num_loc
    
    # Concatenate all features
    node_features = torch.cat([
        locs,  # (batch, N, 2)
        degrees.unsqueeze(-1).float(),  # (batch, N, 1)
        adjacency.float(),  # (batch, N, N) - full adjacency rows
        step_norm.unsqueeze(1).expand(-1, num_loc, 1),  # (batch, N, 1)
        deletion_norm.unsqueeze(1).expand(-1, num_loc, 1),  # (batch, N, 1)
    ], dim=-1)
    
    return node_features
```
    
    return torch.tensor(actions)  # (batch, 3)
```


MASKING FUNCTIONS:
-----------------
```
def compute_add_mask(td):
    # Returns (batch, N, N) boolean mask
    # True = action is valid
    
    adjacency = td['adjacency']  # (batch, N, N)
    degrees = td['degrees']  # (batch, N)
    
    batch, N, _ = adjacency.shape
    
    # Initialize mask as all True
    mask = torch.ones(batch, N, N, dtype=torch.bool)
    
    # Mask existing edges
    mask = mask & (adjacency == 0)
    
    # Mask where either node has degree 2
    degree_2 = (degrees == 2)  # (batch, N)
    mask = mask & ~degree_2.unsqueeze(2)  # broadcast to (batch, N, N)
    mask = mask & ~degree_2.unsqueeze(1)
    
    # Mask self-loops (diagonal)
    mask = mask & ~torch.eye(N, dtype=torch.bool).unsqueeze(0)
    
    # Mask lower triangle (enforce i < j)
    mask = mask & torch.triu(torch.ones(N, N, dtype=torch.bool), diagonal=1).unsqueeze(0)
    
    return mask


def compute_delete_mask(td):
    # Returns (batch, max_edges) boolean mask
    
    selected_edges = td['selected_edges']  # (batch, max_edges, 2)
    num_edges = td['num_edges']  # (batch,)
    starting_edge = td['starting_edge']  # (batch, 2)
    
    batch, max_edges, _ = selected_edges.shape
    
    # Initialize mask
    mask = torch.zeros(batch, max_edges, dtype=torch.bool)
    
    for b in range(batch):
        n = num_edges[b]
        mask[b, :n] = True  # Valid edges
        
        # Find and mask starting edge
        start_i, start_j = starting_edge[b]
        for e in range(n):
            edge_i, edge_j = selected_edges[b, e]
            if (edge_i == start_i and edge_j == start_j) or \
               (edge_i == start_j and edge_j == start_i):
                mask[b, e] = False  # Mask starting edge
                break
    
    return mask


def compute_done_mask(td):
    # Returns (batch,) boolean mask
    
    degrees = td['degrees']  # (batch, N)
    adjacency = td['adjacency']  # (batch, N, N)
    
    batch = degrees.shape[0]
    mask = torch.zeros(batch, dtype=torch.bool)
    
    for b in range(batch):
        # Check if all degrees are 2
        if not (degrees[b] == 2).all():
            continue  # Invalid, keep mask False
        
        # Check connectivity (BFS)
        if is_connected_batch(adjacency[b]):
            mask[b] = True
    
    return mask
```


================================================================================
PART 3: TRAINING SPECIFICATION
================================================================================

ALGORITHM: REINFORCE with Baseline (POMO-style)
-----------------------------------------------

HYPERPARAMETERS:
---------------
- Batch size: 64 problem instances
- POMO augmentation: K = 8 trajectories per instance
  * Effective batch size: 64 * 8 = 512 trajectories
- Optimizer: Adam
  * Learning rate: 1e-4
  * Weight decay: 1e-5
  * LR schedule: Cosine annealing with warmup
    - Warmup: 1000 steps
    - Max LR: 1e-4
    - Min LR: 1e-6
- Gradient clipping: max_norm = 1.0
- Entropy bonus: 0.01 * H(π) (encourage exploration)


DELETE BIAS SCHEDULE:
---------------------
```
# Initial: strongly discourage deletions
# Final: no bias (model decides freely)

delete_bias(epoch) = -5.0 * max(0, 1 - epoch / warmup_epochs)

where warmup_epochs = 0.3 * total_epochs

# Example: If total_epochs = 100
# - Epoch 0: bias = -5.0 (deletions very unlikely)
# - Epoch 15: bias = -2.5
# - Epoch 30: bias = 0.0 (no bias)
# - Epoch 100: bias = 0.0
```


CURRICULUM LEARNING:
-------------------
Progressive training on increasing problem sizes:

Phase 1: N=20 nodes
  - Epochs: 0-50
  - Focus: Learn basic edge selection and deletion mechanics
  - Expected: Model learns to build valid tours

Phase 2: N=50 nodes
  - Epochs: 50-150
  - Focus: Scale to realistic problem sizes
  - Expected: Model learns efficient deletion usage

Phase 3: N=100 nodes (optional)
  - Epochs: 150-250
  - Focus: Test scalability
  - Expected: Maintain solution quality on larger instances

Note: When transitioning, optionally fine-tune for 5-10 epochs on mixed sizes


TRAINING LOOP:
-------------
```
def train_epoch(policy, env, optimizer, batch_size, pomo_size):
    policy.train()
    
    # Generate batch of problems
    td = env.reset(batch_size=batch_size)
    
    # POMO augmentation: expand to K trajectories
    td = expand_for_pomo(td, pomo_size)  # (batch*pomo_size, ...)
    
    # Rollout trajectories
    log_probs = []
    while not td['done'].all():
        # Get action from policy
        td = policy(td, phase='train', decode_type='sampling')
        
        # Step environment
        td = env.step(td)
        
        # Collect log probability
        log_probs.append(td['log_prob'])
    
    # Compute rewards
    rewards = td['reward']  # (batch*pomo_size,)
    
    # Reshape for POMO baseline
    rewards = rewards.view(batch_size, pomo_size)  # (batch, pomo_size)
    log_probs = torch.stack(log_probs).view(-1, batch_size, pomo_size)  # (T, batch, pomo)
    
    # Baseline: mean reward across POMO trajectories
    baseline = rewards.mean(dim=1, keepdim=True)  # (batch, 1)
    
    # Advantage
    advantage = rewards - baseline  # (batch, pomo_size)
    
    # REINFORCE loss
    # Sum log probs over trajectory
    sum_log_probs = log_probs.sum(dim=0)  # (batch, pomo_size)
    
    loss = -(advantage * sum_log_probs).mean()
    
    # Entropy bonus (optional, for exploration)
    entropy = compute_entropy(log_probs)
    loss = loss - 0.01 * entropy.mean()
    
    # Backprop
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=1.0)
    optimizer.step()
    
    # Metrics
    metrics = {
        'loss': loss.item(),
        'reward_mean': rewards.mean().item(),
        'reward_best': rewards.max(dim=1)[0].mean().item(),  # Best per instance
        'entropy': entropy.mean().item(),
    }
    
    return metrics
```


VALIDATION:
----------
```
def validate(policy, env, batch_size, pomo_size):
    policy.eval()
    
    with torch.no_grad():
        # Generate validation problems
        td = env.reset(batch_size=batch_size)
        td = expand_for_pomo(td, pomo_size)
        
        # Greedy rollout
        while not td['done'].all():
            td = policy(td, phase='val', decode_type='greedy')
            td = env.step(td)
        
        rewards = td['reward'].view(batch_size, pomo_size)
        
        # Select best trajectory per instance
        best_rewards = rewards.max(dim=1)[0]
        
        # Additional metrics
        tour_lengths = -best_rewards  # Reward is negative tour length
        num_deletions = td['num_deletions'].view(batch_size, pomo_size).max(dim=1)[0]
        validity_rate = (td['valid_tour'].view(batch_size, pomo_size).any(dim=1)).float().mean()
        
    metrics = {
        'val_tour_length_mean': tour_lengths.mean().item(),
        'val_tour_length_std': tour_lengths.std().item(),
        'val_deletions_mean': num_deletions.float().mean().item(),
        'val_validity_rate': validity_rate.item(),
    }
    
    return metrics
```


================================================================================
PART 4: IMPLEMENTATION CHECKLIST
================================================================================

Phase 1: Environment (Week 1)
------------------------------
[ ] 1.1 CustomTSPEnv class
    - Inherits from rl4co.envs.RL4COEnvBase
    - reset() method with POMO initialization
    - step() method for ADD/DELETE/DONE
    - Reward computation
    - Validity checking (degree + connectivity)

[ ] 1.2 State representation
    - TensorDict with all required fields
    - Node feature computation (polar coordinates)
    - Action history embedding
    - Efficient adjacency matrix updates

[ ] 1.3 Action masking
    - compute_add_mask() function
    - compute_delete_mask() function
    - compute_done_mask() function
    - Tested on edge cases

[ ] 1.4 POMO initialization
    - Diverse edge selection algorithm
    - Starting edge tracking
    - Batch support

[ ] 1.5 Unit tests (custom_tsp_env_test.py)
    - Test reset()
    - Test step() for each action type
    - Test masking functions
    - Test validity checking
    - Test reward computation
    - Test POMO initialization
    
    COMPLETED: 23 tests passing (test_custom_tsp_env.py)
    - Initialization (4): env creation, reset state, zero init, initial mask
    - ADD actions (3): single edge, multiple edges, degree-2 masking
    - DELETE actions (2): single deletion, delete+re-add
    - DONE action (3): valid tour, incomplete tour masking, step limit
    - Masking (3): existing edges, degree-2 nodes, DELETE availability
    - Rewards (2): valid tour length, deletion penalties
    - Edge cases (3): batch independence, N=3, mask consistency
    - Warnings (3): degree>2 detection, invalid DONE, disconnected graph

[ ] 1.6 Visualization (for debugging)
    - Plot TSP instance
    - Show selected edges (green)
    - Show deleted edges (red, faded)
    - Animate episode as GIF
    - Display step counter, deletions, validity
    
    COMPLETED: visualization/plot_tour.py + 6 tests
    - plot_tsp_instance(): Static plots with edge coloring
    - visualize_state(): TensorDict state with metrics overlay
    - create_episode_gif(): Animated GIF from action sequence
    - create_test_episode_gif(): Quick random episode GIF
    - Color coding: green=selected, red=deleted, node colors by degree
    - Metrics: step counter, edge count, deletions, degree labels
    - Test outputs: 4 GIFs + 2 PNGs in tests/test_outputs/


Phase 2: Policy Network (Week 2)
---------------------------------
[ ] 2.1 TransformerEncoder
    - 6-layer transformer
    - Multi-head self-attention (8 heads)
    - Feed-forward networks
    - Layer normalization
    - Residual connections

[ ] 2.2 AddEdgeDecoder
    - Edge embedding computation (concat + distance)
    - Pairwise scoring for all edges
    - Upper triangle extraction (i < j)
    - Masking integration

[ ] 2.3 DeleteEdgeDecoder
    - Edge embedding for selected edges
    - Delete bias application
    - Masking integration

[ ] 2.4 DoneDecoder
    - Global context pooling
    - Single logit for DONE
    - Masking integration

[ ] 2.5 CustomPOMOPolicy class
    - Inherits from appropriate rl4co Policy base
    - forward() method
    - Action decoding (index → action tuple)
    - Log probability computation
    - Delete bias scheduling

[ ] 2.6 Unit tests (custom_pomo_policy_test.py)
    - Test encoder output shapes
    - Test each decoder independently
    - Test full forward pass
    - Test action decoding
    - Test with masked actions
    - Test delete bias effect


Phase 3: Training (Week 3)
---------------------------
[ ] 3.1 REINFORCE trainer
    - Rollout collection
    - Baseline computation (POMO mean)
    - Loss computation
    - Gradient clipping
    - Optimizer step

[ ] 3.2 Delete bias scheduler
    - Epoch-dependent bias
    - Warmup period (30% of training)
    - Integration with policy

[ ] 3.3 Curriculum learning
    - Phase transitions (N=20 → 50 → 100)
    - Problem size scheduling
    - Optional fine-tuning on mixed sizes

[ ] 3.4 Training script (train_custom_pomo.py)
    - Argument parsing (config)
    - Logging setup (tensorboard/wandb)
    - Checkpoint saving/loading
    - Validation loop
    - Metric tracking

[ ] 3.5 Callbacks (if using Lightning)
    - Visualization callback (plot best tours)
    - Metric logging callback
    - Checkpoint callback
    - Early stopping (optional)


Phase 4: Evaluation (Week 4)
-----------------------------
[ ] 4.1 Baseline implementations
    - Nearest neighbor heuristic
    - Farthest insertion heuristic
    - Standard POMO (if available in rl4co)
    - Concorde solver (optional, for optimality gap)

[ ] 4.2 Evaluation script (eval_custom_pomo.py)
    - Load trained model
    - Test on various problem sizes
    - Compare to baselines
    - Statistical significance tests
    - Generate result tables

[ ] 4.3 Visualization tools
    - Plot solution quality vs. baseline
    - Plot deletions used vs. problem size
    - Plot learning curves
    - Animate example solutions


Phase 5: Experiments & Analysis (Ongoing)
-----------------------------------------
[ ] 5.1 Hyperparameter tuning
    - Learning rate
    - Delete bias schedule
    - Entropy bonus weight
    - POMO size K

[ ] 5.2 Ablation studies
    - Effect of polar coordinate features
    - Effect of action history
    - Effect of delete bias scheduling
    - Effect of deletion penalty magnitude

[ ] 5.3 Scalability tests
    - N = 20, 50, 100, 200, 500
    - Inference time measurements
    - Solution quality trends

[ ] 5.4 Generalization tests
    - Train on N=50, test on N=100
    - Different distributions (clustered, uniform, etc.)


================================================================================
PART 5: LOGGING & DEBUGGING STRATEGY
================================================================================

LOGGING LEVELS:
--------------
Use Python logging module with these levels:

- DEBUG: Step-by-step state changes, action details
- INFO: Epoch metrics, validation results
- WARNING: Unusual behavior (e.g., no valid actions, high invalidity rate)
- ERROR: Exceptions, invalid state transitions

LOG FILE STRUCTURE:
------------------
```
logs/
  training_YYYYMMDD_HHMMSS.log       # Main training log
  validation_YYYYMMDD_HHMMSS.log     # Validation results
  environment_debug.log               # Environment state changes (if needed)
```

KEY METRICS TO LOG:
------------------
Per epoch:
  - Loss (REINFORCE)
  - Mean reward (across batch)
  - Best reward (best POMO trajectory)
  - Mean tour length
  - Mean deletions used
  - Validity rate (should be 100% with DONE masking)
  - Entropy (exploration measure)
  - Learning rate
  - Delete bias value

Per validation:
  - Tour length (mean, std, min, max)
  - Optimality gap (if Concorde available)
  - Comparison to baselines
  - Deletions used (mean, std)
  - Steps to termination (mean, std)

Per trajectory (debug mode):
  - Each action taken: (step, action_type, node_i, node_j)
  - State after each action: degrees, num_edges, num_deletions
  - Masked actions at each step
  - Reward components at termination


DEBUGGING CHECKLIST:
-------------------
If model not learning:
  [ ] Check masking: are valid actions available?
  [ ] Check rewards: are they in reasonable range?
  [ ] Check gradients: vanishing or exploding?
  [ ] Check delete bias: too strong initially?
  [ ] Check POMO: are trajectories diverse?
  [ ] Visualize: are tours valid? Are deletions used?

If tours invalid:
  [ ] Check DONE masking: is it too permissive?
  [ ] Check connectivity checking: BFS correct?
  [ ] Check degree tracking: accurate updates?
  [ ] Log invalid tour examples for inspection

If too many deletions:
  [ ] Check deletion penalty: too small?
  [ ] Check delete bias: decaying too fast?
  [ ] Check rewards: is deletion beneficial?

If too few deletions (never used):
  [ ] Check delete bias: too strong?
  [ ] Check deletion penalty: too large?
  [ ] Check masking: are deletions properly unmasked?


================================================================================
PART 6: CODE ORGANIZATION
================================================================================

Directory Structure:
-------------------
```
tsp_custom/
  ├── DESIGN_QUESTIONS1.TXT          # Initial brainstorming
  ├── DESIGN_QUESTIONS2.txt          # Refined questions & answers
  ├── DESIGN_ARCHITECTURE.txt        # This file (finalized design)
  ├── USER_DEVELOPMENT_PLAN.txt      # User's development roadmap
  │
  ├── envs/
  │   ├── __init__.py
  │   ├── custom_tsp_env.py          # CustomTSPEnv class
  │   └── utils.py                   # Helper functions (BFS, distance, etc.)
  │
  ├── models/
  │   ├── __init__.py
  │   ├── custom_pomo_policy.py      # CustomPOMOPolicy class
  │   ├── encoder.py                 # TransformerEncoder
  │   ├── decoders.py                # AddEdgeDecoder, DeleteEdgeDecoder, DoneDecoder
  │   └── utils.py                   # Action decoding, masking functions
  │
  ├── training/
  │   ├── __init__.py
  │   ├── trainer.py                 # REINFORCE training loop
  │   ├── curriculum.py              # Curriculum learning scheduler
  │   └── callbacks.py               # Visualization, logging callbacks
  │
  ├── baselines/
  │   ├── __init__.py
  │   ├── nearest_neighbor.py        # NN heuristic
  │   ├── farthest_insertion.py     # FI heuristic
  │   └── concorde_wrapper.py        # Concorde interface (optional)
  │
  ├── visualization/
  │   ├── __init__.py
  │   ├── plot_tour.py               # Plot TSP solution, animate episodes
  │   └── README.md                  # Visualization usage guide
  │
  │   IMPLEMENTED FUNCTIONS:
  │   - plot_tsp_instance(locs, adjacency, deleted_edges): Static plot
  │   - visualize_state(td, batch_idx): TensorDict visualization
  │   - create_episode_gif(locs, actions, path): Animated episode
  │   - create_test_episode_gif(env, output_path): Quick test GIF
  │
  │   DEBUGGING USAGE:
  │   # Visualize current state during training/testing
  │   from visualization.plot_tour import visualize_state
  │   fig = visualize_state(td, batch_idx=0, save_path='debug_state.png')
  │
  │   # Create episode GIF from rollout
  │   from visualization.plot_tour import create_episode_gif
  │   # Track actions as: [(step, action_type, node_i, node_j), ...]
  │   create_episode_gif(td['locs'][0], action_log, 'episode.gif', fps=2)
  │
  │   COLOR SCHEME (for debugging):
  │   - Green edges: Currently selected (in tour)
  │   - Red dashed: Previously deleted (shows mistakes)
  │   - Light blue nodes: Degree < 2 (incomplete)
  │   - Green nodes: Degree = 2 (valid)
  │   - Red nodes: Degree > 2 (INVALID - check masking!)
  │   - Node labels show: index and [degree]
  │
  │   OUTPUT LOCATIONS:
  │   - Test outputs: tests/test_outputs/*.gif, *.png
  │   - Training: logs/visualizations/ (if callback enabled)
  │   - Manual: specify path in save_path parameter
  │
  ├── tests/
  │   ├── __init__.py
  │   ├── plot_tour.py               # Plot TSP solution
  │   ├── animate_episode.py         # Create GIF of episode
  │   └── plot_metrics.py            # Training curves, comparisons
  │
  ├── tests/
  │   ├── __init__.py
  │   ├── test_custom_tsp_env.py     # Environment unit tests
  │   ├── test_custom_pomo_policy.py # Policy unit tests
  │   ├── test_masking.py            # Masking function tests
  │   └── test_integration.py        # End-to-end tests
  │
  ├── configs/
  │   ├── train_n20.yaml             # Config for N=20 training
  │   ├── train_n50.yaml             # Config for N=50 training
  │   └── train_n100.yaml            # Config for N=100 training
  │
  ├── scripts/
  │   ├── train.py                   # Main training script
  │   ├── eval.py                    # Evaluation script
  │   └── visualize_results.py       # Generate plots and tables
  │
  ├── logs/                          # Training logs (gitignore)
  ├── checkpoints/                   # Model checkpoints (gitignore)
  ├── results/                       # Evaluation results, plots
  │
  └── README.md                      # Project overview, usage instructions
```


Coding Principles (from USER_DEVELOPMENT_PLAN.txt):
---------------------------------------------------
- Variable names: descriptive, clear purpose
- Code readability: prioritize over cleverness
- Comments: 
  * Block comments for sections
  * Inline comments for complex logic
  * Avoid redundant line-by-line comments
- Logging: heavy usage, reference during debugging
- Unit tests: external scripts, not inline error checking
- Error checking: minimal within main code (keep it compact)


================================================================================
PART 7: OPEN QUESTIONS & FUTURE CONSIDERATIONS
================================================================================

Remaining Design Questions (addressed in USER answers):
-------------------------------------------------------
✓ Q1: Polar coordinate reference frame → Absolute, East=0°
✓ Q2: Connection ordering → Closer neighbor first
✓ Q3: Distance features → Absolute (problem already normalized)

Questions for Future Phases:
----------------------------
1. Edge embedding architecture:
   - Start with concat + distance
   - Consider more complex interactions if needed

2. Action history encoding:
   - Start with embed + sum
   - Consider positional encoding if temporal order matters

3. Validation baselines:
   - Implement NN and FI heuristics
   - Compare to standard POMO if available in rl4co
   - Optionally integrate Concorde for optimality gap

4. Hard limit penalty timing:
   - Apply only if tour invalid at 2N steps
   - Monitor if this needs adjustment

5. Connectivity check optimization:
   - Check only when all degrees == 2
   - Use efficient BFS implementation
   - Consider caching or incremental tracking if needed


Future Extensions (beyond initial implementation):
--------------------------------------------------
1. Adaptive deletion penalty:
   - Learn penalty weight as hyperparameter
   - Or schedule penalty during training

2. Hierarchical edge selection:
   - For very large N (500+)
   - Cluster nodes, select edges within clusters first

3. Transfer learning:
   - Pre-train on smaller N, fine-tune on larger
   - Test zero-shot generalization

4. Multi-task learning:
   - Train on TSP + CVRP simultaneously
   - Share encoder, separate decoders

5. Interpretability:
   - Attention visualization: which edges get high scores?
   - Deletion patterns: when/where does model delete?
   - Feature importance: which features most useful?


Potential Risks & Mitigation:
-----------------------------
Risk 1: Model never learns to use deletions effectively
  → Mitigation: Careful delete bias scheduling, deletion penalty tuning
  → Fallback: Allow deletions but don't force them

Risk 2: Invalid tours despite masking
  → Mitigation: Rigorous testing of validity checking
  → Fallback: Add penalty for invalid tours, keep DONE masking

Risk 3: Computational cost too high for large N
  → Mitigation: Efficient implementations, sparse attention
  → Fallback: Limit to N ≤ 100, or use hierarchical approach

Risk 4: Worse than standard POMO
  → Mitigation: Strong baselines, ablation studies
  → Insight: Even if not better, understanding why is valuable

Risk 5: Training instability (high variance)
  → Mitigation: Gradient clipping, entropy bonus, curriculum learning
  → Fallback: Reduce learning rate, increase batch size


================================================================================
FINAL NOTES
================================================================================

This architecture is FINALIZED and ready for implementation.

Next steps (from USER_DEVELOPMENT_PLAN.txt):
  ✓ Step 1: Design architecture finalized (this document)
  → Step 2: Begin implementation of Phase 1 (CustomTSPEnv)

Key principles to maintain:
  - Use rl4co native classes wherever possible
  - Log extensively for debugging
  - Visualize early and often
  - Test each component independently
  - Ask critical questions, not sycophantic

Development approach:
  - Iterative: build minimal viable components first
  - Test-driven: write tests alongside implementation
  - Visual: create visualizations to understand behavior
  - Metrics-focused: track everything that matters

Success criteria:
  - Model learns to build valid TSP tours
  - Model uses deletions strategically (not never, not always)
  - Solution quality competitive with baselines
  - Scales to at least N=100 nodes
  - Code is maintainable and extensible


================================================================================
DOCUMENT VERSION HISTORY
================================================================================

Version 1.0 - 2025-11-22
  - Initial finalized architecture
  - Incorporates all design decisions from Q&A sessions
  - Ready for Step 2 of USER_DEVELOPMENT_PLAN.txt

Future updates should be appended here with date and changes.

================================================================================
END OF DESIGN_ARCHITECTURE.txt
================================================================================


================================================================================
IMPLEMENTATION UPDATES - Step 6 (Model/Policy)
================================================================================
Date: 2025-01-XX (from conversation summary)
Status: COMPLETED

Files Created:
-------------
1. tsp_custom/models/encoder.py (184 lines)
   - TransformerEncoderLayer: Single layer with multi-head self-attention + FFN
   - TransformerEncoder: 6-layer stack with initial projection
   - Architecture: embed_dim=128, num_heads=8, feedforward_dim=512
   - Normalization: instance norm (configurable: batch/instance/layer)
   - Residual connections and layer normalization throughout

2. tsp_custom/models/decoders.py (268 lines)
   - AddEdgeDecoder: Scores N²/2 potential edges using node embeddings + distance
   - DeleteEdgeDecoder: Scores existing edges with delete_bias scheduling
   - DoneDecoder: Global pooling → MLP for termination decision
   - All decoders use similar MLP structure for consistency

3. tsp_custom/models/action_utils.py (187 lines)
   - decode_action_index(): Maps flat action index to (type, i, j)
   - extract_edge_list(): Converts adjacency matrix to edge list for DELETE decoder
   - compute_action_masks(): Splits environment mask for 3 decoder heads
   - create_node_features(): Builds feature vector (locs + degree + adjacency + counters)

4. tsp_custom/models/custom_pomo_policy.py (331 lines)
   - CustomPOMOPolicy: Main policy class combining encoder + 3 decoders
   - forward() method: encode → 3 decoders → concatenate → mask → sample/argmax
   - Delete bias scheduling: -5.0 → 0.0 over 100 epochs
   - Supports train/val/test phases
   - Compatible with rl4co training patterns

5. tsp_custom/models/__init__.py (37 lines)
   - Exports all model components for easy imports

6. tsp_custom/tests/test_policy.py (201 lines)
   - Comprehensive tests for policy forward pass
   - Integration tests with CustomTSPEnv
   - Tests sampling, greedy, delete_bias scheduling
   - All tests passing ✓

Model Architecture Summary:
--------------------------
Input: TensorDict with state (locs, adjacency, degrees, counters)
  ↓
Create node features: (batch, N, N+5)
  - locs (2), degree (1), adjacency row (N), step_norm (1), deletion_norm (1)
  ↓
TransformerEncoder: 6 layers, 128 dim, 8 heads
  ↓
Node embeddings: (batch, N, 128)
  ↓
Three decoder heads:
  1. AddEdgeDecoder → logits_add (batch, N*(N-1)/2)
  2. DeleteEdgeDecoder → logits_del (batch, max_edges)
  3. DoneDecoder → logits_done (batch, 1)
  ↓
Concatenate: (batch, N*(N-1)/2 + max_edges + 1)
  ↓
Apply tanh clipping: logits * tanh(logits/10)
  ↓
Apply action mask from environment
  ↓
Temperature scaling: logits / temperature
  ↓
Softmax + Sample/Argmax
  ↓
Output: action index (batch,)

Key Design Decisions:
--------------------
1. Feature Engineering:
   - Use full adjacency rows (N features) instead of polar coordinates
   - Simplifies implementation, effective for N ≤ 100
   - Future: Can upgrade to polar coordinates if needed

2. Action Space Layout:
   - [ADD actions (N²/2) | DELETE actions (N) | DONE (1)]
   - Total: N²/2 + N + 1 actions (e.g., 211 for N=20)
   - Environment provides combined mask, policy splits for decoders

3. Delete Bias Scheduling:
   - Start: -5.0 (strongly discourage deletion)
   - End: 0.0 (neutral)
   - Warmup: 100 epochs linear schedule
   - Rationale: Learn to build tours before learning to delete

4. Encoder Architecture:
   - 6 layers (POMO default from attention model)
   - Instance norm (better for variable-size batches)
   - FFN dim 512 (2x hidden scaling)
   - No graph context (following POMO paper findings)

5. Decoder Architecture:
   - Symmetric MLPs for all heads (embed_dim*2+1 → 128 → 1)
   - AddEdgeDecoder: All potential edges scored
   - DeleteEdgeDecoder: Only existing edges scored
   - DoneDecoder: Global mean pooling for termination signal

Model Statistics:
----------------
Parameters: 1,264,515 (~1.26M)
Breakdown:
  - Encoder: ~1.1M (6 transformer layers)
  - Decoders: ~150K (3 MLP heads)
Memory per forward pass (batch=64, N=20): ~500MB

Test Results:
------------
✓ Encoder test: Input (4,20,10) → Output (4,20,128)
✓ AddEdgeDecoder test: Outputs (4,190) logits for N=20
✓ DeleteEdgeDecoder test: Outputs (4,20) logits, pads with -inf
✓ DoneDecoder test: Outputs (4,1) logit
✓ Action utils test: Correctly decodes ADD/DELETE/DONE actions
✓ Policy forward test: Samples valid actions from mask
✓ Integration test: Works with CustomTSPEnv step()

Integration with Environment:
-----------------------------
1. Environment provides TensorDict with:
   - State: locs, adjacency, degrees, counters
   - action_mask: Boolean mask (batch, total_actions)

2. Policy processes:
   - Creates node features from state
   - Encodes with transformer
   - Decodes with 3 heads
   - Applies mask and samples

3. Policy returns:
   - action: Flat action index (batch,)
   - log_prob: Log probability (batch,)
   - (optional) action_components: Decoded (type, i, j)

4. Environment steps:
   - Receives action index
   - Decodes to (type, i, j) using utils.decode_action()
   - Updates state accordingly

Next Steps (Step 7 - Unit Tests):
---------------------------------
✓ Step 6 completed successfully
→ Step 7: Create comprehensive unit tests for model
  - Test encoder with various input sizes
  - Test each decoder head independently
  - Test policy with edge cases (no feasible actions, etc.)
  - Test gradient flow through full forward pass
  - Test delete_bias scheduling
  - Test integration with POMO multi-start (future)

Future Enhancements:
-------------------
1. Multi-start POMO:
   - Implement num_starts parameter
   - Initialize with diverse edge sets
   - Select best trajectory

2. Polar coordinate features:
   - Replace adjacency rows with polar coords
   - Reduces feature dim from N+5 to ~15
   - May improve generalization

3. Context embedding:
   - Add global context to decoder (mean/max pooling)
   - May improve DELETE/DONE decisions

4. Attention visualization:
   - Extract attention weights from encoder
   - Visualize which nodes/edges model focuses on

================================================================================


================================================================================
STEP 7: COMPREHENSIVE MODEL UNIT TESTS - COMPLETED
================================================================================
Date: November 22, 2025
Status: ✓ COMPLETED
File: tsp_custom/tests/test_model_comprehensive.py (789 lines)

Overview:
--------
Comprehensive unit testing suite for all model components following Step 7
of USER_DEVELOPMENT_PLAN.txt. Tests focus on passing known states to the
model/policy and verifying valid action outputs, along with edge case testing
and gradient flow validation.

Test Suite Contents:
-------------------

TEST 1: Encoder with Various Input Sizes
  - Tests: N=10, 20, 50, 100
  - Validates: Output shapes, no NaN/Inf, embedding statistics
  - Results: ✓ All sizes work correctly
  - Parameter counts:
    * N=10:  1,188,608 params
    * N=20:  1,189,888 params
    * N=50:  1,193,728 params
    * N=100: 1,200,128 params
  - Output normalization: mean≈0.0, std≈1.0 (instance norm working)

TEST 2: AddEdgeDecoder Edge Cases
  - Test Case 1: Standard input (N=20, 190 edges)
  - Test Case 2: Small graph (N=5, 10 edges)
  - Test Case 3: Logit statistics (range checking)
  - Validates: Edge pair ordering (i < j), no NaN outputs
  - Results: ✓ All edge pairs correctly generated, logits reasonable

TEST 3: DeleteEdgeDecoder Edge Cases
  - Test Case 1: With existing edges (proper scoring)
  - Test Case 2: No deletable edges (all -inf)
  - Test Case 3: Delete bias impact (-5.0, -2.5, 0.0)
  - Validates: Padding mask works, bias adds correctly
  - Results: ✓ Padding positions = -inf, bias scheduling works
  - Observed: Bias shifts logits by expected amount

TEST 4: DoneDecoder Edge Cases
  - Test Case 1: Standard input
  - Test Case 2: Various graph sizes (N=5,10,20,50)
  - Validates: Output shape (batch, 1), no NaN
  - Results: ✓ Consistent output across different N

TEST 5: Policy Edge Cases
  - Test Case 1: Single feasible action per batch
    * Forces greedy to select only feasible action
    * Results: ✓ All batches select correct action
  
  - Test Case 2: High degree nodes (most nodes degree=2)
    * Only low-degree nodes can receive ADD actions
    * Results: ✓ Policy respects degree constraints
  
  - Test Case 3: Valid tour state (can use DONE)
    * Hamiltonian cycle with all degree=2
    * Results: ✓ DONE and DELETE both feasible, samples correctly

TEST 6: Gradient Flow
  - Setup: 3 batches with different action types
    * Batch 0: ADD actions feasible
    * Batch 1: DELETE actions feasible (has edges)
    * Batch 2: DONE action feasible
  - Validates: Encoder gradients always present
  - Results: ✓ Encoder components have non-zero gradients
  - Note: Decoder gradients may be zero if actions not sampled (expected)
  - Gradient clipping test: ✓ Works correctly with max_norm=1.0

TEST 7: Delete Bias Scheduling
  - Configuration: start=-5.0, end=0.0, warmup=100
  - Test epochs: 0, 25, 50, 75, 100, 150
  - Validates: Linear interpolation, val phase always 0.0
  - Results: ✓ All epochs match expected values
  - Custom schedule test: start=-10.0, end=5.0, warmup=50
    * Results: ✓ Custom parameters work correctly

TEST 8: Integration with Environment (Multi-Step)
  - Environment: CustomTSPEnv with N=15, batch=2
  - Takes 10 steps with policy sampling
  - Validates: State updates correctly, rewards computed
  - Results: ✓ Policy + environment integration seamless
  - Observations:
    * After 10 steps: ~10 edges added, 0 deletions
    * Degree stats: min=0, max=2, mean≈1.33
    * Episodes not finished (expected for 10 steps)

TEST 9: Action Utility Functions
  - decode_action_index(): Tests ADD/DELETE/DONE decoding
    * Results: ✓ All action types decoded correctly
  
  - extract_edge_list(): Tests adjacency → edge list conversion
    * Results: ✓ Edges extracted, padding = -1
  
  - create_node_features(): Tests feature vector construction
    * Expected shape: (batch, N, N+5)
    * Features: locs(2) + degree(1) + adjacency(N) + counters(2)
    * Results: ✓ All features present and normalized

Key Findings:
------------
1. Model Architecture:
   - Encoder scales well from N=10 to N=100
   - Parameter count grows modestly with N (only projection layer)
   - Instance normalization maintains stable embeddings (mean≈0, std≈1)

2. Decoder Behavior:
   - AddEdgeDecoder: Produces reasonable logit ranges
   - DeleteEdgeDecoder: Correctly applies bias and padding
   - DoneDecoder: Consistent across different graph sizes

3. Policy Behavior:
   - Always respects action masks (critical for validity)
   - Greedy selection works deterministically
   - Sampling explores feasible actions correctly

4. Gradient Flow:
   - Encoder receives gradients from all action types
   - Decoders only receive gradients when their actions sampled
   - Gradient clipping works for RL training stability

5. Integration:
   - Policy seamlessly integrates with CustomTSPEnv
   - State → features → embeddings → actions pipeline works
   - Multi-step rollouts execute without errors

Test Coverage Summary:
---------------------
✓ Encoder: 4 input sizes tested
✓ AddEdgeDecoder: 3 edge cases tested
✓ DeleteEdgeDecoder: 3 edge cases + bias scheduling
✓ DoneDecoder: 2 test cases + size variations
✓ Policy: 3 edge cases (single action, high degree, valid tour)
✓ Gradient flow: Full backward pass validated
✓ Delete bias: 6 epochs + custom schedule tested
✓ Environment integration: 10-step rollout validated
✓ Action utilities: 3 functions tested comprehensively

Total: 9 major test categories, all passing

Comparison with Step 4 Environment Tests:
-----------------------------------------
Step 4 tested environment in isolation (29 tests)
Step 7 tests model+environment integration (9 comprehensive tests)
Together: Full coverage of environment, model, and their interaction

Next Steps (Step 8):
-------------------
✓ Step 7 completed successfully
→ Step 8: Develop train.py script
  - Integrate with rl4co training loop
  - Implement REINFORCE + POMO multi-start
  - Add baseline (rollout or learned value function)
  - Configure hyperparameters (lr, batch_size, epochs)
  - Add logging and checkpointing
  - Create training callbacks for visualization

Ready for Training:
------------------
Model: ✓ Fully implemented and tested
Environment: ✓ Fully implemented and tested
Integration: ✓ Validated with multi-step rollouts
Visualization: ✓ Available from Step 5
Metrics: ✓ Environment tracks costs and validity

All prerequisites for Step 8 (train.py) are now complete.

================================================================================


================================================================================
STEP 8: TRAINING SCRIPT - COMPLETED
================================================================================
Date: November 22, 2025
Status: ✓ COMPLETED
Files Created:
- tsp_custom/train.py (281 lines)
- tsp_custom/models/custom_pomo_model.py (430 lines)
- tsp_custom/tests/test_training_setup.py (120 lines)

Overview:
--------
Implemented comprehensive training script following Step 8 of USER_DEVELOPMENT_PLAN.txt.
The training infrastructure integrates CustomTSPEnv, CustomPOMOPolicy, and PyTorch Lightning
to enable REINFORCE-based training with POMO-style multi-start rollouts.

Key Components:
--------------

1. CustomPOMOModel (custom_pomo_model.py)
   - Extends PyTorch Lightning's LightningModule
   - Implements REINFORCE with shared baseline (POMO-style)
   - Handles sequential episode rollouts until done
   - Integrates delete bias scheduling
   - Supports both rl4co and standalone modes
   
   Key Methods:
   * shared_step(): Common logic for train/val/test
   * on_train_epoch_end(): Updates delete bias schedule
   * configure_optimizers(): Sets up Adam optimizer
   
   Training Loop (shared_step):
   1. Reset environment with batch
   2. Roll out episode with policy until all done
   3. Collect log probabilities at each step
   4. Compute rewards from final state
   5. Calculate REINFORCE loss with shared baseline
   6. Return loss and metrics

2. train.py Script
   - Command-line interface with argparse
   - Comprehensive configuration options
   - PyTorch Lightning Trainer integration
   - Checkpointing and logging setup
   - Support for TensorBoard and W&B logging
   
   Command-line Arguments:
   * Environment: --num_loc (default: 20)
   * Model: --embed_dim, --num_encoder_layers, --num_heads, --feedforward_dim
   * Delete bias: --delete_bias_start, --delete_bias_end, --delete_bias_warmup_epochs
   * Training: --batch_size, --max_epochs, --lr, --weight_decay
   * Data: --train_data_size, --val_data_size, --test_data_size
   * Logging: --log_dir, --experiment_name, --use_wandb
   * Checkpointing: --checkpoint_dir, --save_top_k
   * Hardware: --accelerator, --devices, --gradient_clip_val
   * Other: --seed, --resume_from_checkpoint

3. test_training_setup.py
   - Smoke tests for training pipeline
   - Verifies all components integrate correctly
   - Tests: environment, policy, model, forward pass, episode rollout, training step
   - All tests passing ✓

Training Pipeline:
-----------------

Data Flow:
1. Generate random TSP instances (locs)
2. Reset environment with batch
3. Policy selects actions sequentially:
   - Encode state with TransformerEncoder
   - Score actions with 3 decoder heads
   - Apply mask and sample/greedy select
4. Environment steps with action
5. Repeat until all episodes done
6. Compute rewards (negative tour length)
7. REINFORCE loss with shared baseline
8. Backpropagate and update policy

REINFORCE with Shared Baseline:
  Loss = -E[(R - baseline) * log π(a|s)]
  
  Where:
  - R: episode reward (negative tour length)
  - baseline: mean reward across batch (shared baseline)
  - log π(a|s): sum of log probabilities over episode
  - E[·]: expectation over batch

Delete Bias Scheduling:
  - Epoch 0: bias = -5.0 (strongly discourage deletion)
  - Epoch t: bias = (1 - t/T) * (-5.0) + (t/T) * 0.0
  - Epoch T (100): bias = 0.0 (neutral)
  - Rationale: Learn to build tours before learning to delete

Usage Examples:
--------------

Basic Training:
  python tsp_custom/train.py --num_loc 20 --max_epochs 100 --batch_size 64

Quick Test (small scale):
  python tsp_custom/train.py --num_loc 10 --max_epochs 5 --batch_size 32 \
    --train_data_size 1000 --val_data_size 100

Full Training (POMO settings):
  python tsp_custom/train.py \
    --num_loc 20 \
    --max_epochs 100 \
    --batch_size 64 \
    --train_data_size 1280000 \
    --val_data_size 10000 \
    --lr 1e-4 \
    --weight_decay 1e-6 \
    --delete_bias_warmup_epochs 100 \
    --save_top_k 3 \
    --use_wandb \
    --wandb_project tsp_custom

Resume Training:
  python tsp_custom/train.py --resume_from_checkpoint path/to/checkpoint.ckpt

GPU Training:
  python tsp_custom/train.py --accelerator gpu --devices 1

Multi-GPU Training:
  python tsp_custom/train.py --accelerator gpu --devices 4

Integration with PyTorch Lightning:
-----------------------------------

Callbacks Implemented:
1. ModelCheckpoint: Save best models based on val/reward
2. RichModelSummary: Print model architecture
3. LearningRateMonitor: Track learning rate schedule

Logging:
- TensorBoard (default): Logs to tsp_custom/lightning_logs/
- Weights & Biases (optional): --use_wandb flag

Metrics Logged:
- train/loss: REINFORCE loss
- train/reward: Mean episode reward
- train/tour_length: Mean tour length
- train/baseline_val: Baseline value
- train/log_likelihood: Mean log probability
- val/reward: Validation reward
- val/tour_length: Validation tour length
- test/reward: Test reward
- test/tour_length: Test tour length

Data Management:
---------------

Dataset Generation:
- Training: New random instances every epoch
- Validation: Fixed set (seed=1234) for consistent comparison
- Test: Fixed set (seed=5678) for final evaluation

Dataset Sizes (default):
- Training: 1,280,000 instances (matches POMO paper)
- Validation: 10,000 instances
- Test: 10,000 instances

Batch Sizes (default):
- Training: 64
- Validation: 1024 (larger for faster evaluation)
- Test: 1024

DataLoader:
- num_workers=4 (parallel data loading)
- persistent_workers=True (keep workers alive between epochs)
- reload_dataloaders_every_n_epochs=1 (generate new data each epoch)

Test Results:
------------

✓ Test 1: Environment Creation
  - CustomTSPEnv created successfully
  - Generator produces valid problem instances

✓ Test 2: Policy Creation
  - CustomPOMOPolicy created successfully
  - Parameters: 204,291 (N=10, small config)
  - Parameters: ~1,265,000 (N=20, full config)

✓ Test 3: Model Creation
  - CustomPOMOModel created successfully
  - Integrates with PyTorch Lightning

✓ Test 4: Forward Pass
  - Policy produces valid actions
  - Action and log_prob shapes correct

✓ Test 5: Full Episode Rollout
  - Episodes complete successfully
  - All instances reach done state
  - Rewards computed correctly

✓ Test 6: Training Step
  - Loss computed successfully
  - Backward pass works
  - No NaN or Inf values

✓ Test 7: Delete Bias Scheduling
  - Bias schedules correctly from -5.0 to 0.0
  - Linear interpolation verified

Architecture Decisions:
----------------------

1. Sequential Rollout vs. Autoregressive:
   - Unlike standard TSP models, we use sequential environment steps
   - Each step: encode state → decode action → step → repeat
   - More flexible than autoregressive node selection
   - Allows DELETE and DONE actions

2. Shared Baseline (POMO):
   - Simple and effective: baseline = mean(rewards)
   - No critic network needed
   - Works well for RL with deterministic environments

3. Delete Bias Curriculum:
   - Start with strong negative bias (-5.0)
   - Gradually reduce to neutral (0.0)
   - Allows model to learn construction first, then refinement

4. PyTorch Lightning Integration:
   - Automatic GPU/multi-GPU support
   - Checkpointing and logging handled
   - Easy to extend with callbacks
   - Compatible with rl4co if installed

Comparison with Standard POMO:
-----------------------------

Similarities:
- Shared baseline for REINFORCE
- Adam optimizer (lr=1e-4, weight_decay=1e-6)
- Instance normalization in encoder
- Gradient clipping (max_norm=1.0)
- Dataset regeneration every epoch

Differences:
- Sequential action selection vs. autoregressive
- 3 action types (ADD/DELETE/DONE) vs. single (select node)
- Delete bias scheduling (curriculum learning)
- State includes adjacency matrix (graph topology)
- No multi-start during training (yet - can add in future)

Known Limitations:
-----------------

1. No POMO multi-start during training:
   - Currently uses single rollout per instance
   - Could add multi-start for better exploration
   - Future enhancement for Step 9

2. Simple baseline:
   - Shared baseline (mean of batch)
   - Could use learned critic or rollout baseline
   - Future enhancement if training unstable

3. No augmentation:
   - Could add dihedral-8 augmentation (8 symmetries)
   - Standard for TSP to improve generalization
   - Future enhancement

4. Fixed delete bias schedule:
   - Linear schedule over 100 epochs
   - Could make adaptive based on deletion rate
   - Future enhancement

Next Steps (Step 9):
-------------------
✓ Step 8 completed successfully
→ Step 9: Develop callbacks for visualization during validation
  - Create visualization callback that generates GIFs
  - Plot training/validation metrics (costs/distances)
  - Save outputs to checkpoint directory
  - Implement epoch## filename prefix for organization

Ready for Training:
------------------
All components implemented and tested:
✓ Environment (CustomTSPEnv)
✓ Policy (CustomPOMOPolicy)
✓ Model (CustomPOMOModel)
✓ Training script (train.py)
✓ Integration tests passing

Can now run full training:
  python tsp_custom/train.py --num_loc 20 --max_epochs 100 --batch_size 64

Expected Training Time:
- N=20, 100 epochs, batch_size=64: ~2-4 hours on single GPU
- N=50, 100 epochs, batch_size=64: ~8-12 hours on single GPU
- N=100, 100 epochs, batch_size=32: ~24-48 hours on single GPU

================================================================================
